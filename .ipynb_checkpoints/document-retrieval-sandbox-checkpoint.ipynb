{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc2e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d9a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee8e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8ecbcf",
   "metadata": {},
   "source": [
    "# Get Started\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**LangChain** is a framework for developing applications powered by language models. It enables applications that:\n",
    "\n",
    "+ **Are context-aware**: connect a langulate model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.)\n",
    "+ **Reason**: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.)\n",
    "\n",
    "The main values props of LangChain are:\n",
    "\n",
    "1. **Components**: abstractions for working with language models\n",
    "2. **Off-the-shelf chains**: a structured assembly of components for accomplishing specific higher-level tasks\n",
    "\n",
    "### Installation\n",
    "\n",
    "#### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ff020",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66be6315",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install typing_extensions==4.7.1 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c9015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f707ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key='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')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b6e1c5-8ba3-4d2a-a429-4ba25de20145",
   "metadata": {},
   "source": [
    "#### Building an Application\n",
    "\n",
    "LangChain provides modules for building language model applications. Modules can be combined for more complex use cases. The most common and import chain LangChain helps create contains three things:\n",
    "\n",
    "+ **LLM**: the core reasoning engine is the language model.\n",
    "+ **Prompt Tempaltes**: instructions to the language model.\n",
    "+ **Output Parsers**: translate the raw response from the LLM for use downstream\n",
    "\n",
    "#### LLMs\n",
    "\n",
    "+ **LLMs**: a language model that takes a string as input and returns a string\n",
    "+ **ChatModels**: a language model that takes a list of messages as input and returns a message\n",
    "\n",
    "A `ChatMessage` has 2 required components:\n",
    "\n",
    "+ `content`: content of the message\n",
    "+ `role`: role of the enrity the `ChatMessage` is coming from\n",
    "\n",
    "LangChain has several objects for distinguising roles:\n",
    "\n",
    "+ `HumanMessage`: a `ChatMessage` from a human\n",
    "+ `AIMessage`: a `ChatMessage` from an AI\n",
    "+ `SystemMessage`:a `ChatMessage` from the system\n",
    "+ `FuntionMessage`: a `ChatMessage` from a function\n",
    "\n",
    "LangChain has a standard interface with two methods:\n",
    "\n",
    "+ `predict`: takes a string and returns a string\n",
    "+ `predict_messages`: takes a list of messages and returns a message\n",
    "\n",
    "##### Import an LLM and a ChatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adde374b-68bd-4f5e-8f1d-fe2615acc72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e2f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c8ab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f58ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.predict(\"hi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef79c22f-4131-46e0-8b09-e255b40e5587",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00dd182-87fd-4b61-9d36-8b6818b70f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model.predict(\"hi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72c7ddb-573f-4348-8fda-3da05ba6c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"What would be a good company name for a speculative quantitative trader's fund?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d103ec-73e8-4ab9-8e5f-d10541f5539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519dd8a8-7eaa-4672-8523-a88e01cf2afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model.predict(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed68b8b1-8b04-45be-a754-b22a2a6fb0f1",
   "metadata": {},
   "source": [
    "Let's use the `predict_messages` method to run over a list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68870b39-5c3c-4ab3-943c-e921221161ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1265ede-5642-467b-8a72-89f799e46804",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf2bea0-7721-423b-b265-4a1eac8bdc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdf7fab-3681-4e04-bc9f-c93b33394d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b52c317-cc30-4bfa-9a5f-2ca5b81fc61c",
   "metadata": {},
   "source": [
    "#### Prompt Templates\n",
    "\n",
    "Most LLM applications do not pass user input directly into an LLM. Usually, LLM applications will add the user input into a larger piece of text called a prompt template that provides additional context on the specific task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f04cd7-202d-48a9-be88-871667ca51e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6316063-baee-430d-b635-7a69991f70dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc87c50-4f31-4505-ae36-3422a8db4388",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.format(product='financial returns uncorrelated with the broader market')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f897c053-ff7a-4a62-a25b-ff0e08b985fd",
   "metadata": {},
   "source": [
    "For more explanation on prompt template functionality, see the LangChain [section on prompts](https://python.langchain.com/docs/modules/model_io/prompts).\n",
    "\n",
    "PromptTemplates can also be used to produce a list of messages. What happens most often is a ChatPromptTemplate is a list of ChatMessageTemplates. Each ChatMessageTemplate containes instructions for how to format a ChatMessage (i.e., role, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39fb6a68-2bb6-44ed-9448-4a5aa447b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ace638c-a9d8-40c9-aa00-aac69e3c980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "human_template = \"{text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd3e17b-3bc3-424c-8d46-e544d3f00d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa237ff8-f982-4764-819a-b6cd2a8a0072",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt.format_messages(input_language=\"English\", \n",
    "                            output_language=\"Spanish\", \n",
    "                            text=\"I love programming.\"\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaae23d2-0570-44be-b8c2-798071cc3364",
   "metadata": {},
   "source": [
    "#### Output Parsers\n",
    "\n",
    "OutputParsers convert the raw output of an LLM into a format that can be used downstream. There are a few main types:\n",
    "\n",
    "+ Convert text from LLM -> structured information (i.e., json)\n",
    "+ Convert a ChatMessage into a string\n",
    "+ Convert the extra information returned from a call besides the message (like OpenAI function invocation) into a string\n",
    "\n",
    "For more information, see the [section on output parsers](https://python.langchain.com/docs/modules/model_io/output_parsers)\n",
    "\n",
    "Let's convert a comma seperated list into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180f77ae-529f-4764-bd24-e7d15df2ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7891dc-2549-4560-a536-812f9d3e581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommaSeperatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\" Parse the output of an LLM call to a comma-seperated list. \"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\" Parse the output of an LLM call. \"\"\"\n",
    "        return text.strip().split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a364ad-0e23-48b2-9c15-a5c0ac68fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CommaSeperatedListOutputParser().parse(\"hi, bye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f683f9-7c9a-4b88-adc6-f0fd7bbd2d72",
   "metadata": {},
   "source": [
    "### PromptTemplate + LLM + OutputParser\n",
    "\n",
    "We can combine all 3 components into one chain. The chain will take input variables, pass those to a prompt template to create a tempalte, pass the prompt to a language model, then pass the output through an output parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efaff2f-e2af-4074-a50a-cdb1f4a295ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42624e4e-f2c7-4ce5-84c2-1f490787bc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommaSeperatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\" Parse the output of an LLM call to a comma-seperated list. \"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\" Parse the output of an LLM call. \"\"\"\n",
    "        return text.strip().split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08432f1-0c19-4592-a0e3-51cfcaca9179",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a helpful assistant who generates comma seperate lists. A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more.\"\"\"\n",
    "human_template = \"{text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff858fe-b615-4b60-8b70-e06e869d421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beccf81-ce94-426d-8205-afddd4161b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt | ChatOpenAI() | CommaSeperatedListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c6e23d-8f3d-49f9-b0ea-3cedddfc5064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chain.invoke({\"text\": \"colors\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7f0dfe-b30d-4fdd-94f8-fdc178935a32",
   "metadata": {},
   "source": [
    "The `|` syntax joins these components together. The `|` syntax is called the LangChain expression Language. To learn more, check out the documentation [here](https://python.langchain.com/docs/expression_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd87c04",
   "metadata": {},
   "source": [
    "# LangChain Expression Language\n",
    "\n",
    "## Cookbook for Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f26e1f-f5d0-4033-897c-f8c76e933281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/andkelly/miniforge3/lib/python3.9/site-packages (0.0.320)\n",
      "Requirement already satisfied: openai in /Users/andkelly/miniforge3/lib/python3.9/site-packages (0.27.0)\n",
      "Requirement already satisfied: faiss-cpu in /Users/andkelly/miniforge3/lib/python3.9/site-packages (1.7.4)\n",
      "Requirement already satisfied: tiktoken in /Users/andkelly/miniforge3/lib/python3.9/site-packages (0.2.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from langchain) (1.4.49)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from langchain) (3.8.6)\n",
      "Requirement already satisfied: anyio<4.0 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from langchain) (3.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from langchain) (0.6.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from langchain) (0.0.43)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from langchain) (2.4.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from langchain) (8.1.0)\n",
      "Requirement already satisfied: tqdm in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: blobfile>=2 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from tiktoken) (2.0.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from tiktoken) (2022.10.31)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from anyio<4.0->langchain) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from anyio<4.0->langchain) (1.1.1)\n",
      "Requirement already satisfied: pycryptodomex~=3.8 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from blobfile>=2->tiktoken) (3.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from blobfile>=2->tiktoken) (2.0.6)\n",
      "Requirement already satisfied: lxml~=4.9 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from blobfile>=2->tiktoken) (4.9.2)\n",
      "Requirement already satisfied: filelock~=3.0 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from blobfile>=2->tiktoken) (3.8.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from pydantic<3,>=1->langchain) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from pydantic<3,>=1->langchain) (4.7.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/andkelly/miniforge3/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain openai faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67101b8a-ee41-43f1-9401-384f7a4b56d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0d8d9de-236b-4f6e-96f3-ed8a5befdbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_texts([\"andrew works at a consulting firm\"],\n",
    "                               embedding=OpenAIEmbeddings()\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e26edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d499ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" Answer the question based only on the following context: {context}\n",
    "\n",
    "Questions: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6c84056",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40322853-56f7-4c81-83b0-70a04582fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3257cc-1222-4696-9dd7-80319a5d2872",
   "metadata": {},
   "source": [
    "The \"retriever\" (i.e., corpus of documents) sets the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c57f5bf7-cb25-4fe4-a0fb-75570f29243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae0a25c9-9faf-49a8-86f9-021b5da468a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Andrew worked at a consulting firm.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"where did andrew work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b49483-ee5c-4cff-b28d-6bfc9f1ae421",
   "metadata": {},
   "source": [
    "### Conversational Retrieval Chain\n",
    "\n",
    "We can add in conversation history. This means adding in `chat_message_history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ea4404b-06d4-4f88-8a83-16c4b74492be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableMap\n",
    "from langchain.schema import format_document\n",
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "772dc9c4-3cfe-4e9c-8556-291f2a6d9a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "934c158a-eb17-4cf8-ba06-c073a4a36e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68094d78-26e9-4bef-aa84-0be239c745d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context: \n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49f5265c-a7d7-4741-a5d8-e875319c39f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "603a8987-3e03-4ed4-9539-5de3220c6b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65ba11fd-97ee-43aa-9506-d916339587c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _combine_documents(docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff408f66-ea85-47a0-812e-e47a55941fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c692dc8f-176d-446c-ac80-37786f3f8e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_chat_history(chat_history: List[Tuple]) -> str:\n",
    "    buffer = \"\"\n",
    "    for dialogue_turn in chat_history:\n",
    "        human = \"Human: \" + dialogue_turn[0]\n",
    "        ai = \"Assistant: \" + dialogue_turn[1]\n",
    "        buffer += \"\\n\" + \"\\n\".join([human, ai])\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dffca0dd-ee48-4b6c-afe1-0f9c4b4df830",
   "metadata": {},
   "outputs": [],
   "source": [
    "_inputs = RunnableMap(\n",
    "    standalone_question = RunnablePassthrough.assign(\n",
    "        chat_history = lambda x: _format_chat_history(x['chat_history'])\n",
    "    ) | CONDENSE_QUESTION_PROMPT | ChatOpenAI(temperature=0) | StrOutputParser(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98f20bc9-712b-4042-9ea5-255930872b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a0ba3d0-f64c-4e58-8a8a-1231a398d130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  standalone_question: RunnableAssign(mapper={\n",
      "                         chat_history: RunnableLambda(lambda x: _format_chat_history(x['chat_history']))\n",
      "                       })\n",
      "                       | PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:')\n",
      "                       | ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, temperature=0.0, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')\n",
      "                       | StrOutputParser()\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9170636c-0ee8-4ef5-a79b-8c287a0ba119",
   "metadata": {},
   "outputs": [],
   "source": [
    "_context = {\n",
    "    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n",
    "    \"question\": lambda x: x[\"standalone_question\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08f32a0f-7c6c-4013-a4fa-3fba495c5cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20f42ba4-f413-4dfd-bfe4-a3e2468f8af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Andrew was employed at a consulting firm.')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_qa_chain.invoke({\n",
    "    \"question\": \"where did andrew work?\",\n",
    "    \"chat_history\": [],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb21c46e-1939-4c45-8d56-4070a5c8505c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Andrew worked at a consulting firm.')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_qa_chain.invoke({\n",
    "    \"question\": \"where did he work?\",\n",
    "    \"chat_history\": [\"who wrote this notebook?\", \"Andrew\"],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dbc215-0558-4a0a-afce-e7786d858eb3",
   "metadata": {},
   "source": [
    "## Cookbook for Adding Memory\n",
    "\n",
    "Add memory to an arbitraty chain. You can use memory classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cebfbe3f-3248-4af8-8c1d-c0a7e82b9df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44290ad8-7252-483f-9342-4abc1077246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "649e1807-431e-49ee-8bc8-891bacd75610",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful chatbot\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d93035ab-3cac-45fc-a55e-be5825e3959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b6b0a41-f214-4b2f-bd55-9e19ebe76f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': []}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "efc6d63e-c79b-4154-b579-69e1fbebfea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnablePassthrough.assign(\n",
    "    memory=RunnableLambda(memory.load_memory_variables) | itemgetter('history')\n",
    ") | prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b505e279-9dc1-465a-b53b-91d0e1e1ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"input\": \"hi im bob\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0c4eb5aa-b002-4cf7-8527-45e4e9e6f6e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/langchain/schema/runnable/base.py:1137\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 1137\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/langchain/schema/prompt_template.py:58\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Dict, config: RunnableConfig \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_variables\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/langchain/schema/runnable/base.py:652\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    646\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    648\u001b[0m     run_type\u001b[38;5;241m=\u001b[39mrun_type,\n\u001b[1;32m    649\u001b[0m     name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    650\u001b[0m )\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 652\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    656\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/langchain/schema/runnable/config.py:201\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    200\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/langchain/schema/prompt_template.py:60\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke.<locals>.<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Dict, config: RunnableConfig \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_prompt(\n\u001b[0;32m---> 60\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{key: inner_input[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_variables}\n\u001b[1;32m     61\u001b[0m         ),\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m     63\u001b[0m         config,\n\u001b[1;32m     64\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     65\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/langchain/schema/prompt_template.py:60\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Dict, config: RunnableConfig \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_prompt(\n\u001b[0;32m---> 60\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{key: \u001b[43minner_input\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_variables}\n\u001b[1;32m     61\u001b[0m         ),\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m     63\u001b[0m         config,\n\u001b[1;32m     64\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     65\u001b[0m     )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'history'"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ac63ff-4c2c-42b8-b2f1-f03e589e608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(inputs, {\"output\": response.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d2353a-e449-455c-83b0-9401d594430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853b448-019b-4a34-a93c-ba3dbd6a6b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"input\": \"whats my name\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c971e2bf-d2ef-415f-ac7f-e1ffcb0bf1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f13da6-9cc5-43db-a61d-b63cd8361145",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb23d58f-65df-4aae-bc95-2b1fb66f01d9",
   "metadata": {},
   "source": [
    "#### With Memory and Returing Source Documents\n",
    "\n",
    "For memory, we need to manage outside memory. For returning the retrieved documents, we need to pass them through all the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "677b21d9-c8ca-4023-a60e-7ea0f0b4c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.runnable.base import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8026b174-7f52-4e9a-9764-d462d6ff19d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True, output_key=\"answer\", input_key=\"question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df5c11-42e0-4511-bef6-71d8acd6b383",
   "metadata": {},
   "source": [
    "Potential [solution](https://python.langchain.com/docs/modules/memory/adding_memory_chain_multiple_inputs) to the error when instantiating chat history in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c888b0a-3356-482c-9266-3c627b1d8734",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'dict' and 'operator.itemgetter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_memory_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mitemgetter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhistory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'dict' and 'operator.itemgetter'"
     ]
    }
   ],
   "source": [
    "# memory.load_memory_variables({}) | itemgetter(\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "019117be-84d2-428f-b31b-ef80d5695038",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa6ac89c-40f1-4e41-8a60-33e71a6f9578",
   "metadata": {},
   "outputs": [],
   "source": [
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: _format_chat_history(x['chat_history'])\n",
    "    } | CONDENSE_QUESTION_PROMPT | ChatOpenAI(temperature=0) | StrOutputParser(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58fe8769-4d9f-40cb-af66-2a6d265060fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the documents\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
    "    \"question\": lambda x: x[\"standalone_question\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4869412a-0e24-4371-a160-4554b2c28aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the inputs for the final prompt\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d371db6a-2bdb-42de-b18f-53b3e6cfeb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the answers\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | ChatOpenAI(),\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18b7d80f-3d0f-421a-97dc-54898ae02e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it all together!\n",
    "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eca75558-5504-4ab4-9fa0-e65a93517525",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"question\": \"where did harrison work?\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b8321d5-920b-4f47-9db9-2590f997c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = final_chain.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b12200d1-dcec-4a0a-9482-b6c9f9cf0bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': AIMessage(content=\"There is no information provided about Harrison's employment.\"),\n",
       " 'docs': [Document(page_content='andrew works at a consulting firm')]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb332d3-a60f-4471-ad42-1e12c0d337c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a37171f0",
   "metadata": {},
   "source": [
    "# Modules\n",
    "\n",
    "## Retrievers\n",
    "\n",
    "### Interface with application-specific data\n",
    "\n",
    "LangChain uses [Chroma](https://python.langchain.com/docs/ecosystem/integrations/chroma.html) as the vector store to index and search embeddings. To walk through this tutorial, we'll first need to install `chromadb`. [Link](https://python.langchain.com/docs/modules/data_connection/retrievers#get-started) to the getting started exercise showcasing question answering over documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed48380",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a549aebc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca70ae1b-526d-43e7-8ff1-2b001fed3575",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "### [Getting Started](https://python.langchain.com/docs/modules/memory/)\n",
    "\n",
    "Take a look at how to use `ConversationBufferMemory` in chains. `ConversationBufferMemory` is a \"simple\" form of memory that keeps a list of chat messages in a buffer and passes those into a prompt template.\n",
    "\n",
    "A more complex memory system might return a succint summany of past K messages. A more sophisticated system might extract entities from stored messages and only return information about entities referenced in the current run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a941f-b6d6-4e07-a4d7-99f3378e813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain==0.0.320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c36ca518-2674-415c-9b12-b378c56b398c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain                     0.0.320\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list | grep \"langchain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe52ae04-34b3-471a-9db8-859d836d69f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f07f106-bc8c-43c7-95d7-c974bea58432",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44dc7cbe-395d-4223-b7e9-1fc2fdb0205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.chat_memory.add_user_message(\"hi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe64c52a-8cfa-490a-88c1-d9c7ab860119",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.chat_memory.add_ai_message(\"what's up?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3156ce0f-04ab-461c-96a9-30710f273455",
   "metadata": {},
   "source": [
    "#### Variables returned from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2e07a8-dc58-4f79-9dbc-10b3a2964205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: hi!\\nAI: what's up?\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fab8b2-5d92-4201-872e-05f941779162",
   "metadata": {},
   "source": [
    "If you want the memory variables to be returned in the key `chat_history` you can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afad4762-7dd0-46ea-887b-a771f623fe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61411f65-a1d0-465a-bea7-51e2049f9d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.chat_memory.add_user_message(\"hi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "182eb4a0-5392-459e-97b0-753380bb1e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.chat_memory.add_ai_message(\"what's up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb1fca4f-b2f7-4bf2-839a-81a271f04899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi!'), AIMessage(content=\"what's up?\")]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da18d16-6fec-4f0e-a16e-6c2107e18656",
   "metadata": {},
   "source": [
    "Often, chains return multiple input/output keys. This is contollable by `input_key` and `output_key` parameters on the memory types.\n",
    "\n",
    "#### End to end example\n",
    "\n",
    "##### Using an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6142cbc0-92bc-46a6-9b42-e362ffea099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b1a358e-1e95-4756-8837-1a078a2718f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15dc22e5-f583-465f-940e-3cb6d322d9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice  \"chat_history\" is present in the prompt template\n",
    "template = \"\"\"You are a nice chatbot having a conversation with a human.\n",
    "\n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "\n",
    "New human question: {question}\n",
    "Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b295dd76-ae51-408e-b18a-1dcba9570c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "599130aa-98a1-406e-9612-ec7a7222b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice we need to align the `memory_key`\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    # verbose=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a6dc3cf-53ec-4b30-a3a0-b0db1c35aeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'hi',\n",
       " 'chat_history': '',\n",
       " 'text': ' Hi there! How can I help you?'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notice we pass in the `question` variables - `chat_history` gets populated by memory\n",
    "conversation({\"question\": \"hi\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c032a6f-c301-4ed0-986d-d0564075623f",
   "metadata": {},
   "source": [
    "##### Using a ChatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f221f696-3043-44aa-86b2-ef42169fe373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "688434cd-418f-45bd-8dc3-5175637180a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "982dbb05-3852-466a-8c86-d0b80b63a0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a nice chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        # `variable_name` here is what must align with memory\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd98ed50-3a98-4234-a188-761f20fcea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice we `return_messages=True` to fit into the MessagesPlaceholder\n",
    "# notice `\"chat_history\"` aligns with the MessagesPlaceholder name\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2770d61d-7000-4c61-8eab-0b57b345003b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    # verbose=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2fc2fcad-6749-4a0c-a9f4-c783a13e5ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'hi',\n",
       " 'chat_history': [HumanMessage(content='hi'),\n",
       "  AIMessage(content='Hello! How can I assist you today?')],\n",
       " 'text': 'Hello! How can I assist you today?'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# motice we pass in the `question` variables - `chat_history` gets populated by memory\n",
    "conversation({\"question\": \"hi\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2af0232-0fba-4fc1-94fb-db44ff12a080",
   "metadata": {},
   "source": [
    "### Memory in the Multi-input Chain\n",
    "\n",
    "[Link](https://python.langchain.com/docs/modules/memory/adding_memory_chain_multiple_inputs) to add memory to a chain that has multiple inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e55b0c-0029-47ad-ae64-7953f9aadfbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a61b3072",
   "metadata": {},
   "source": [
    "## Chains\n",
    "\n",
    "### Foundational LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba67c15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e1e6c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a62b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995013a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35817b4d",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) Use Cases\n",
    "\n",
    "## Ouickstart\n",
    "\n",
    "See documentation [here](https://python.langchain.com/docs/use_cases/question_answering/#overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c4701a-7bd0-4e93-915e-85de4dae7cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0b56c2-ada9-4a44-a9ed-4d2a34443bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d303c-2284-4f04-9f46-8bdae45a6922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2233d38-13ad-4d2f-98da-34832ffe76f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f667128-38a2-406b-b261-2698e1e5f594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f200d799-7a76-4a57-a6e4-6738bc18d35e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8014dab0-bc79-4412-83cb-c2e01d4e0b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675889a1-520d-4b37-b8af-bfff8c7071f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e4ba288-bd41-41ab-85d5-89f87aaee01a",
   "metadata": {},
   "source": [
    "## Using a Retriever \n",
    "\n",
    "This [example](https://python.langchain.com/docs/use_cases/question_answering/vector_db_qa) demonstrates question answering over an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd66c351-c01f-4fff-9f64-fcd56faa6200",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install soupsieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eed229-4d1e-4826-a865-63b6888a9787",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f10a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import UnstructuredWordDocumentLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a856b1-cfc0-40b0-bdb6-ae9e81436768",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredWordDocumentLoader(\n",
    "        \"/Users/andkelly/Library/CloudStorage/Dropbox/Applications Folder/Resumes/2023_Kelly_Andrew_Acorns_Resume.doc\", \n",
    "        mode=\"elements\", \n",
    "        strategy=\"fast\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e460e75-c206-4e2b-9378-bcfb94b29a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e694bc32-93f1-4e3e-8f69-a40915615479",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7efd957-2e86-43f1-854e-d229bec58b2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315bb71e-28e1-485e-8196-5e56478f524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c132a60-1cb2-4091-a497-09be2f7fcb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e18d04-4454-4e68-95c5-95ed3dc62bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6cc117-42b4-4b7e-9d6a-eb47b65a5256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25c50e34-b58c-4cb5-9c6b-77fd0d8c7194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a0f056-7cd3-4ce1-b71d-b012f791d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = langchain.vectorstores.utils.filter_complex_metadata(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c54471",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a778ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dae1dfa-c342-40e4-bb0f-17352f56a2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c6092f-b282-4aad-b817-816d4d5eb3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"give me a 2 sentence elevator pitch for Andrew\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0345e8c-c194-41b8-b376-8ce8980f34ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5717c4f5-2fbf-433a-b551-db89a9342b43",
   "metadata": {},
   "source": [
    "### File Directory\n",
    "\n",
    "For each path in this directory, load into the vector store. See [documentation](https://python.langchain.com/docs/modules/data_connection/document_loaders/file_directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980f7c88-cf9b-4ecd-a1f3-312c5a21d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install grpcio==1.58.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6ddb86-3134-4b29-a36a-c5df5d389aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"unstructured[pdf]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c95ffc81-7904-4923-baad-76f26c8e08fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e55bf9a5-8431-4de8-8e49-8f1345257d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('/Users/andkelly/Library/CloudStorage/Dropbox/Applications Folder/Resumes', glob=\"**/*.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0420d018-a88c-49f7-b9e6-408685071731",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2645ca1a-9154-4d54-a38a-c809ed1b24d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbf21018-2ced-4afb-8f22-c628f6736fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "637d6746-4427-4ebf-a163-8b02c0400bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf4547ab-3523-4ca7-a616-4c5174457962",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02aeae78-8534-46ab-ab01-ae626f7a8d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9dc3bab-2110-4ac1-b2d9-e2c7242bb4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f774eff4-06dc-4e8d-85c2-615171302847",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4c30eaf-ceb5-488e-8af3-d88ee613c9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07911ce8-d0b3-4d10-a973-671bf0efcb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4b96bc4-58a0-4fc1-b647-e4a938ad2559",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23fe28da-d8b2-4854-a346-bd7b280903cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9eb6237b-e6e2-4511-bf6d-f77caac49b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6270163-1cfa-477a-9785-0026d5aa784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"give me a 2 sentence elevator pitch for Andrew\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "241c80eb-46fb-4a56-a9ba-b4739742faa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Andrew Arthur Kelly is a Finance professional who graduated with a Master's of Science in International Business from the University of Florida's Hough Graduate School of Business. He also holds a Bachelor of Science in Business Administration from the University of Florida's Warrington College of Business and was a Cum Laude Honors Graduate with a GPA of 3.52.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9213cdbe-6367-409a-978d-ec799052c422",
   "metadata": {},
   "source": [
    "### Vector Store Retriever Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b179fc03-d8ae-46bc-baf9-3bd253ea7c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3433fe01-35bf-4fc0-b1b4-6ee084bcf858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbc73143-8e2e-414e-a4be-457ef049342a",
   "metadata": {},
   "source": [
    "### Return Source Documents\n",
    "\n",
    "Return source documents used to answer the quesion by specifying an optional parameter when constructing the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55831d42-83f7-4bd0-ba7a-a7d4aeba9387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf088da5-3d1d-47cc-afdc-16c7151e5d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0f1cd2c-7df5-4475-94df-82723ef0b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bfc21e-3c3a-49c9-aedd-8d779d29fe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm = OpenAI(), \n",
    "                                 chain_type=\"stuff\", \n",
    "                                 retriever=docsearch.as_retriever(serach_type =\"mmr\",\n",
    "                                                                 search_kwag={\n",
    "                                                                     'fetch_k': 30\n",
    "                                                                 }),\n",
    "                                return_source_documents=True\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91472cd-9bd3-45e5-8a98-049356d13445",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what did andrew design and develop?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630a4a03-6992-44bd-ba8e-3942185ac4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489dab54-66d6-444b-9008-7316109fc9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b24c61d-f1d9-4b86-8ee3-5bddc10bbcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = result[\"source_documents\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b57dbdf-fdc4-4859-bdd2-990888e95b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = test_doc.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a47160b-5015-4df0-b62a-e65fc88fd8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d045a4-571c-42e8-937a-41440ee5d72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc.get('kwargs').get('metadata').get('filename')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef60e2d-1553-4d18-872c-96237ec1ea20",
   "metadata": {},
   "source": [
    "#### Easter egg\n",
    "\n",
    "[documentation](https://api.python.langchain.com/en/latest/api_reference.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e562610-b6e5-4cd6-b8b9-348bd39b56c6",
   "metadata": {},
   "source": [
    "## Remembering Chat History\n",
    "\n",
    "The ConversationalRetrievalQA chain builds on RetrievalQA chain to provide chat history.\n",
    "\n",
    "First, combine the chat history (either explicitly or retrieved from memory) and the question into a standalone question. Next, look up releveant documents from the retriever. Finally, pass those documents and the question to a question-answering chain to return a response.\n",
    "\n",
    "Let's create a retriever from document embeddings in a vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28d9f682-fc43-4f74-a7d1-950984780eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce779915-7df6-4b84-a844-618f94670b1a",
   "metadata": {},
   "source": [
    "Load in documents. This step can be swapped out with a loader for any type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b84029af-aef2-4014-b1a2-dc483a36ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredWordDocumentLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a00392fb-7d12-4b9f-b597-e9ae2c1748dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredWordDocumentLoader(\n",
    "        \"/Users/andkelly/Library/CloudStorage/Dropbox/Essays/Bjarke Ingels.docx\", \n",
    "        mode=\"elements\", \n",
    "        strategy=\"fast\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a569587b-9ec2-4824-99c7-c7f9dc90989b",
   "metadata": {},
   "source": [
    "If we had multiple loaders (i.e., multiple data source formats), you could use the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0048a544-78c9-4535-9386-8eeda54e36b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaders = [....]\n",
    "# docs = []\n",
    "# for loader in loaders:\n",
    "#     docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e114583f-9042-4927-9d76-37c34f5e7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8cd3539-9eba-4946-aa1b-980b3ff083fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8bdd152-ada9-4a55-9a0e-2d5a86f35e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c90e1b5-b720-49f1-90e3-367e3c83a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4139bcb3-c0a9-4608-9878-b17aa6c56705",
   "metadata": {},
   "outputs": [],
   "source": [
    "docuemnts = utils.filter_complex_metadata(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f78ce799-5340-41aa-a9ff-4295f4562c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e304f781-5bce-4ce4-a0bb-dea927898b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8e711a-573f-439d-a69c-21d7c04473ad",
   "metadata": {},
   "source": [
    "Create a memory object to track the inputs/outputs to hold a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c455e34d-fd84-43c6-ba26-ea9df510f468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aafb42e4-38b6-451f-be56-8f27c7bd9f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key = \"chat_history\",\n",
    "                                 return_messages = True\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1e0629f-b987-45df-a9af-ca1644eb323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), \n",
    "                                           vectorstore.as_retriever(), \n",
    "                                           memory=memory\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f28cf99d-7164-4d0b-9116-cf11f7a7e91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is Bjarke Ingels?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb4b3968-7bfb-4685-8d9c-ddc6dc7d15ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ca33e99-bb2e-4a3c-9d02-7f63ed0d683f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Bjarke Ingels is an architect from Denmark who is striving to design and engineer structures that are socially, economically, and environmentally perfect. He believes that architecture is the human manipulation of the surface of the planet to make sure that if fits the way we want to live. He is also interested in creating a social infrastructure for cities.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da35653b-df32-40a7-bd11-c52e8e7a6c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What buildings is he famous for?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41373907-79b7-4176-b976-dec6206e03be",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83361c59-4aaf-4537-8e19-a42c820b8825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Bjarke Ingels has designed the Vancouver House, the Titling Building in Huaxi, five open-air swimming pools at Islands Brygge Harbour Bath in the Copenhagen Harbour, the Maritime Youth House, a sailing club and a youth house at Sundby Harbour in Copenhagen, and the VM Houses in restad, Copenhagen.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa760d0-e75b-4059-84c0-5c448aea68ab",
   "metadata": {},
   "source": [
    "For more examples, revisit the [documentation](https://python.langchain.com/docs/use_cases/question_answering/chat_vector_db#pass-in-chat-history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84977443-13ba-4296-bd02-647e299c3823",
   "metadata": {},
   "source": [
    "## Citing Retrival Sources\n",
    "\n",
    "Use OpenAI functions ability to extract citations from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e467469-eccf-44ae-b4b2-dd1c0d724acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_citation_fuzzy_match_chain\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b64891e-5b9f-47ee-9549-f5bd61ecba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What did the author do during college?\"\n",
    "context = \"\"\"\n",
    "My name is Andrew Kelly, and I grew up in Sarasota Florida but I was born in Michigan.\n",
    "I went to a public highschool but in university I studied finance. \n",
    "As part of a summer internship, I worked at many companies including Citi, University of Florida.\n",
    "I also Chaired Student Government Productions at the University of Florida.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683d90db-ea1d-4b8f-ba7a-a872d9323362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205191a7-6f6d-454b-84e7-215ae9330b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = Model.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a6328b-2f9c-4bbc-a784-db987fb31084",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models[\"data\"]:\n",
    "    print(model[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c2ad1-0d5a-49bb-a671-5337d9ca7510",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0a06d0-e4c7-40c7-b046-33675247bdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = create_citation_fuzzy_match_chain(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3564eb3-9e28-41d1-991d-c03ad81775d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.run(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7786642-c4ab-4249-a216-02d68a19f308",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4352ef2d-4d54-49d2-a630-547516821214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight(text, span):\n",
    "    return(\n",
    "        \"...\"\n",
    "        + text[span[0] - 20 : span[0]]\n",
    "        + \"*\"\n",
    "        + \"\\033[91m\"\n",
    "        + text[span[0] : span[1]]\n",
    "        + \"\\033[0m\"\n",
    "        + \"*\"\n",
    "        + text[span[1] : span[1] + 20]\n",
    "        + \"...\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee2fd7-7e6a-48cf-98e0-9ba3185161bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fact in result.answer:\n",
    "    print(\"Statement:\", fact.fact)\n",
    "    for span in fact.get_spans(context):\n",
    "        print(\"Citation:\", highlight(context, span))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fabba4f",
   "metadata": {},
   "source": [
    "# Integrations\n",
    "\n",
    "## Components\n",
    "\n",
    "### Retriever for Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a393d407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ce2237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
